# -*- coding: utf-8 -*-
"""Text_Encodings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Geq9DCUSekKzyBK2inXH3U212XCbguUh

Downloading Data directly from Internet
"""

! wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz

! ls

!pwd

! tar -zxvf aclImdb_v1.tar.gz

!ls

!ls /content/aclImdb

! cat /content/aclImdb/README

! head - 10 /content/aclImdb/imdb.vocab

! ls /content/aclImdb/train/pos

! cat /content/aclImdb/train/pos/4233_7.txt

# Commented out IPython magic to ensure Python compatibility.
# % cat /content/aclImdb/train/pos/9936_10.txt

"""Get data for pos and neg reviews for train"""

import os

dir = "/content/aclImdb/train/pos"
k=100 # loading data for 100 text files
raw=[]
index_map = dict() 
i =0
for f in os.listdir(dir):
    path = dir + "/" +f
    with open(path,'r') as a:
      data = a.read()
      raw.append(data)
      index_map[i] = a 
    i+=1

    if i == k:
      break

print(i)
print(index_map)
print(raw)

"""Data Preproessing

Stopwords removal
"""

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import nltk
nltk.download('stopwords')
nltk.download('punkt')
  

def remove_stopwords(sentence, stopwords):
  tokens = word_tokenize(sentence)
  filtered_tokens = "";
  for token in tokens:
    if token not in stopwords:
      filtered_tokens+=token
  return filtered_tokens

stopwords = set(stopwords.words("english"))
sentence = raw[0]
print(remove_stopwords(sentence, stopwords))

"""stopword_Lemmatization"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

example_sent = "This is a sample sentence, showing off the stop words filtration."
def stopWord_Lemmatize(sent, stop_words, lemmatizer ):
  word_tokens = word_tokenize(sent) # tokenize
  return_sent = "";
  
  for w in word_tokens:
      if w not in stop_words:
          return_sent = return_sent + " "+lemmatizer.lemmatize(w) # lemmatize w beofre adding it to the return_sent
  return return_sent

stop_words = set(stopwords.words('english')) # NLTK 
lemmatizer = WordNetLemmatizer()

print(raw[0])
returned = stopWord_Lemmatize(raw[0], stop_words, lemmatizer)

preprocessed_data =[]

for sentence in raw:
  preprocessed_data.append(stopwords_lemmetize(sentence,stopwords, lemmatizer))

preprocessed_data[5]

"""Bag of words"""

from sklearn.feature_extraction.text import CountVectorizer

bow = CountVectorizer()

x_bow = bow.fit_transform(preprocessed_data)

print(len(bow.get_feature_names()))

print(type(x_bow))

x_bow.data.nbytes

x_bow_dense = x_bow.todense()

x_bow_dense.data.nbytes

print(x_bow.shape)
print(x_bow_dense.shape)

print(x_bow)

"""TF-IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

vect  = TfidfVectorizer(stop_words="english", ngram_range=(1,2))

tfidf =  vect.fit_transform(preprocessed_data)
print(vect.get_feature_names())

type(tfidf)

tfidf_dense = tfidf.todense()

print(type(tfidf_dense))

print(tfidf.data.nbytes)
print(tfidf_dense.data.nbytes)

"""Word2Vec"""

! wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
! gunzip GoogleNews-vectors-negative300.bin.gz

! ls

pip install --upgrade gensim

from gensim.models import KeyedVectors

filename = "GoogleNews-vectors-negative300.bin"

model = KeyedVectors.load_word2vec_format(filename, binary = True)

len(model)

type(model)

model

"""BERT"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

#https://github.com/hanxiao/bert-as-service

# https://github.com/hanxiao/bert-as-service/issues/380
# Install BERT-SERVING client and Server
!pip install bert-serving-client
!pip install -U bert-serving-server[http]

# dowload pretrained models
!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip uncased_L-12_H-768_A-12.zip

# Start BERT_SERVER on the current computer
!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &

# Use bert-client from python
# Takes time to execute as it uses GPU
from bert_serving.client import BertClient
bc = BertClient()
print (bc.encode(['First do it', 'then do it right', 'then do it better'])) # list of setences

v = bc.encode([raw[0]])

print(v.shape)
print(v)

